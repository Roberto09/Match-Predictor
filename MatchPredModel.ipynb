{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.tabular import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_learner??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Signature: tabular_learner(data:fastai.basic_data.DataBunch, layers:Collection[int], emb_szs:Dict[str, int]=None, metrics=None, ps:Collection[float]=None, emb_drop:float=0.0, y_range:Union[Tuple[float, float], NoneType]=None, use_bn:bool=True, **learn_kwargs)\n",
    "def tabular_learner(data:DataBunch, layers:Collection[int], emb_szs:Dict[str,int]=None, metrics=None,\n",
    "        ps:Collection[float]=None, emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, **learn_kwargs):\n",
    "    \"Get a `Learner` using `data`, with `metrics`, including a `TabularModel` created using the remaining params.\"\n",
    "    emb_szs = data.get_emb_szs(ifnone(emb_szs, {}))\n",
    "    model = TabularModel(emb_szs, len(data.cont_names), out_sz=data.c, layers=layers, ps=ps, emb_drop=emb_drop,\n",
    "                         y_range=y_range, use_bn=use_bn)\n",
    "    return Learner(data, model, metrics=metrics, **learn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_learner??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Dropout??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(Module):\n",
    "    \"Basic model for tabular data.\"\n",
    "    def __init__(self, emb_szs:ListSizes, n_cont:int, out_sz:int, layers:Collection[int], ps:Collection[float]=None,\n",
    "                 emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, bn_final:bool=False):\n",
    "        super().__init__()\n",
    "        ps = ifnone(ps, [0]*len(layers))\n",
    "        ps = listify(ps, layers)\n",
    "        self.embeds = nn.ModuleList([embedding(ni, nf) for ni,nf in emb_szs])\n",
    "        self.emb_drop = nn.Dropout(emb_drop)\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "        self.n_emb,self.n_cont,self.y_range = n_emb,n_cont,y_range\n",
    "        sizes = self.get_sizes(layers, out_sz)\n",
    "        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]\n",
    "        layers = []\n",
    "        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n",
    "            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "        if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def get_sizes(self, layers, out_sz):\n",
    "        return [self.n_emb + self.n_cont] + layers + [out_sz]\n",
    "\n",
    "    def forward(self, x_cat:Tensor, x_cont:Tensor) -> Tensor:\n",
    "        if self.n_emb != 0:\n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_drop(x)\n",
    "        if self.n_cont != 0:\n",
    "            x_cont = self.bn_cont(x_cont)\n",
    "            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
    "        x = self.layers(x)\n",
    "        if self.y_range is not None:\n",
    "            x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Pred Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How will this model work?\n",
    "\n",
    "Well, in simple terms we will have shared weights for A and B inpus where A and B are the exact same information but for the 2 different contestants, this information can be as big as desired. Then we will bind in the general stats such as weather, time and related stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchTabularModel(nn.Module):\n",
    "    \"Basic model for match tabular data.\"\n",
    "    def __init__(self,\n",
    "                 #Contestants parameters\n",
    "                 emb_szs_cts:ListSizes, n_cont_cts:int, layers_cts:Collection[int], ps_cts:Collection[float]=None, emb_drop_cts:float=0.,\n",
    "                 #General Data parameters\n",
    "                 emb_szs_grl:ListSizes, n_cont_grl:int, layers_grl:Collection[int], ps_grl:Collection[float]=None, emb_drop_grl:float=0.,\n",
    "                 #General Model parameters\n",
    "                 use_bn:bool=True, bn_final:bool=False, out_sz:int, y_range:OptRange=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"Contestants Setup\"\"\"\n",
    "        # Dropout probabilities\n",
    "        ps_cts = ifnone(ps_cts, [0]*len(layers_cts))\n",
    "        ps_cts = listify(ps_cts, layers_cts)\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeds_cts = nn.ModuleList([embedding(ni, nf) for ni,nf in emb_szs_cts])\n",
    "        self.emb_drop_cts = nn.Dropout(emb_drop_cts)\n",
    "        n_emb_cts = sum(e.embedding_dim for e in self.embeds_cts)\n",
    "        \n",
    "        # Continious (non-embedding)\n",
    "        self.bn_cont_cts = nn.BatchNorm1d(n_cont_cts)\n",
    "        \n",
    "        # Embeddings and Continious\n",
    "        self.n_emb_cts, self.n_cont_cts = n_emb_cts, n_cont_cts\n",
    "        sizes_cts = self.get_sizes(n_emb_cts, n_cont_cts, layers_cts)\n",
    "        actns_cts = [nn.ReLU(inplace=True) for _ in range(len(sizes_cts)-1)]\n",
    "        layers_cts = []\n",
    "        for i,(n_in, n_out, dp, act) in enumerate(zip(sizes_cts[:-1], sizes_cts[1:], [0.]+ps_cts, actns_cts)):\n",
    "            layers_cts += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "        self.layers_cts = layers_cts    \n",
    "        \n",
    "        \"\"\"General Setup\"\"\"\n",
    "        # Dropout Probabilities\n",
    "        ps_grl = ifnone(ps_grl, [0]*len(layers_grl))\n",
    "        ps_grl = listify(ps_grl, layers_grl)\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeds_grl = nn.ModuleList([embedding(ni, nf) for ni,nf in emb_szs_grl])\n",
    "        self.emb_drop_grl = nn.Dropout(emb_drop_grl)\n",
    "        n_emb_grl = sum(e.embedding_dim for e in self.embeds_grl)\n",
    "        \n",
    "        # Continious (non-embedding)\n",
    "        n_cont_grl += sizes_cts[-1] # adding output from contestants subnet to the size of the continious input\n",
    "        self.bn_cont_grl = nn.BatchNorm1d(n_cont_grl)\n",
    "        \n",
    "        # Embeddings and Continitous\n",
    "        self.n_emb_grl, self.n_cont_grl = n_emb_grl, n_cont_grl\n",
    "        sizes_grl = self.get_sizes(n_emb_grl, n_cont_grl, layers_grl, out_sz)\n",
    "        actns_grl = [nn.ReLU(inplace=True) for _ in range(len(sizes_grl)-2)] + [None]\n",
    "        layers_grl = []\n",
    "        for i,(n_in, n_out, dp, act) in enumerate(zip(sizes_grl[:-1], sizes_grl[1:], [0.]+ps_grl, actns_grl)):\n",
    "            layers_grl += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "        \n",
    "        if bn_final: layers_grl.append(nn.BatchNorm1d(sizes_grl[-1]))\n",
    "        self.layers_grl = nn.Sequential(*layers_grl)\n",
    "    \n",
    "        \"\"\"General Model Setup\"\"\"\n",
    "        self.y_range = y_range\n",
    "\n",
    "    def get_sizes(self, n_emb, n_cont, layers, out_sz = None):\n",
    "        res = [n_emb + n_cont] + layers\n",
    "        return  res + [out_sz] if out_sz is not None else res\n",
    "\n",
    "    def forward(self,\n",
    "                # Contestant A\n",
    "                cat_a:Tensor, cont_a:Tensor,\n",
    "                # Contestant B\n",
    "                cat_b:Tensor, cont_b:Tensor,\n",
    "                # Genearal data\n",
    "                cat_grl:Tensor, cont_grl:Tensor) -> Tensor:\n",
    "        \n",
    "        # Forward of Contestant Layers:\n",
    "        if self.n_emb_cts != 0:\n",
    "            a = [e(cat_a[:,i]) for i,e in enumerate(self.embeds_cts)]\n",
    "            b = [e(cat_a[:,i]) for i,e in enumerate(self.embeds_cts)]\n",
    "            \n",
    "            a = torch.cat(a, 1)\n",
    "            b = torch.cat(b, 1)\n",
    "            \n",
    "            # Not very sure about this step, check here and in layers.\n",
    "            # TODO: implement later: https://discuss.pytorch.org/t/how-to-fix-the-dropout-mask-for-different-batch/7119/3\n",
    "            #a = a.emb_drop_cts(a)\n",
    "            #b = b.emb_drop_cts(b)\n",
    "        \n",
    "        if self.n_cont_cts != 0:\n",
    "            # not sure about this step, check here and in layers\n",
    "            # TODO: check best practice for the batchnorm (use the same normalization for both or not)\n",
    "            cont_a = self.bn_cont_cts(cont_a)\n",
    "            cont_b = self.bn_cont_cts(cont_b)\n",
    "            \n",
    "            a = torch.cat([a, cont_a], 1) if self.n_emb_cts != 0 else cont_a\n",
    "            b = torch.cat([b, cont_b], 1) if self.n_emb_cts != 0 else cont_b\n",
    "            \n",
    "        for lyr in self.layers_cts:\n",
    "            a = lyr(a)\n",
    "            b = lyr(b)\n",
    "        \n",
    "        # SUM\n",
    "        x = a+b\n",
    "        \n",
    "        if self.n_emb_grl != 0:\n",
    "            x_cat = [e(cat_grl[:,i]) for i,e in enumerate(self.embeds_grl)]\n",
    "            x_cat = torch.cat(x_cat, 1)\n",
    "            x_cat = self.emb_drop_grl(x_cat)\n",
    "            x = torch.cat([x, x_cat], 1)\n",
    "        if self.n_cont != 0:\n",
    "            x_cont = self.bn_cont_grl(cont_grl)\n",
    "            x = torch.cat([x, x_cont], 1) if self.n_emb_grl != 0 else x_cont\n",
    "            \n",
    "        x = self.layers_grl(x)\n",
    "        if self.y_range is not None:\n",
    "            x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "listify??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 a\n",
      "1 2 b\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = ['a', 'b', 'c']\n",
    "\n",
    "for i, (x, y) in enumerate(zip(a[:-1], b)):\n",
    "    print(i, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `_DropoutNd` not found.\n"
     ]
    }
   ],
   "source": [
    "._DropoutNd??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `nn._DropoutNd` not found.\n"
     ]
    }
   ],
   "source": [
    "nn._DropoutNd??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = Tensor([[1, 1, 1],\n",
    "           [1, 1, 1],\n",
    "           [1, 1, 1]])\n",
    "\n",
    "drp = nn.Dropout(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BatchNorm1d??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

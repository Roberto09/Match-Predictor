{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/ligamx/WLT2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['L_ES_LOC'] = True\n",
    "df['V_ES_LOC'] = False\n",
    "\n",
    "df.at[0, 'L_ES_LOC'] = False\n",
    "df.at[0, 'V_ES_LOC'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = df.iloc[range(143, 161)].copy()\n",
    "valid_df = df.iloc[range(125, 143)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4901"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.index[range(161, 174)], inplace = True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_idx(cdf):\n",
    "    cdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.index[range(125, 161)], inplace = True)\n",
    "fix_idx(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_extra_2019 = df[df.ANO == 2019].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "# df_extra_2019_apertura = df[df.ANO == 2019][df.TIPO == 'apertura'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([df_extra_2019_apertura, df_extra_2019, df])\n",
    "# df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_og = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF for training\n",
    "df = pd.concat([df_og, valid_df])\n",
    "fix_idx(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF for testing\n",
    "df_t = pd.concat([df_og, valid_df, testing_df])\n",
    "fix_idx(df_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_emb_sz_2(classes, n, sz_dict=None):\n",
    "    \"Pick an embedding size for `n` depending on `classes` if not given in `sz_dict`.\"\n",
    "    print(classes[n])\n",
    "    sz_dict = ifnone(sz_dict, {})\n",
    "    n_cat = len(classes[n])\n",
    "    sz = sz_dict.get(n, int(tabular.data.emb_sz_rule(n_cat)))  # rule of thumb\n",
    "    return n_cat,sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular.data.def_emb_sz = def_emb_sz_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df\n",
    "deep_vars = ['RES']\n",
    "#categorical values\n",
    "cat_names = ['LOC', 'L_ES_LOC', 'VIS', 'V_ES_LOC', 'JRD', 'ANO', 'TIPO',]\n",
    "#continious variables\n",
    "cont_names = ['L_JJ', 'L_POS', 'V_POS',\n",
    "       'L_R_JG_JJ', 'L_R_JE_JJ', 'L_R_JP_JJ', 'L_R_GF_JJ', 'L_R_GC_JJ',\n",
    "       'L_R_DIF_JJ', 'L_R_PTS_JJ', 'V_JJ', 'V_R_JG_JJ', 'V_R_JE_JJ',\n",
    "       'V_R_JP_JJ', 'V_R_GF_JJ', 'V_R_GC_JJ', 'V_R_DIF_JJ', 'V_R_PTS_JJ']\n",
    "procs = [Categorify, Normalize]\n",
    "\n",
    "data = (TabularList.from_df(df2, path=\"./data/ligamx\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "   .split_none()\n",
    "   .label_from_df(cols=deep_vars)\n",
    "   .databunch(bs=2))\n",
    "\n",
    "data.get_emb_szs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_lmx = df\n",
    "data_separated = []\n",
    "for i in range(2012, 2020):\n",
    "    data_separated.append(pd.concat([data_lmx[data_lmx.ANO == i][data_lmx.TIPO == 'clausura'], data_lmx[data_lmx.ANO == i][data_lmx.TIPO == 'apertura']]))\n",
    "\n",
    "sub_data = pd.concat([data_separated[6], data_separated[7]])\n",
    "    \n",
    "val_rng = range(155, 175)\n",
    "\n",
    "for i, dataf in enumerate(data_separated):\n",
    "    if i < 7: data_separated[i] = (pd.concat([dataf, data_lmx.iloc[val_rng]]))\n",
    "    data_separated[i].reset_index(drop=True, inplace=True)\n",
    "data_separated[0].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "deep_vars = ['RES']\n",
    "#categorical values\n",
    "cat_names = ['LOC', 'VIS', 'JRD', 'ANO', 'TIPO']\n",
    "#continious variables\n",
    "cont_names = ['L_JJ', 'L_POS', 'V_POS',\n",
    "       'L_R_JG_JJ', 'L_R_JE_JJ', 'L_R_JP_JJ', 'L_R_GF_JJ', 'L_R_GC_JJ',\n",
    "       'L_R_DIF_JJ', 'L_R_PTS_JJ', 'V_JJ', 'V_R_JG_JJ', 'V_R_JE_JJ',\n",
    "       'V_R_JP_JJ', 'V_R_GF_JJ', 'V_R_GC_JJ', 'V_R_DIF_JJ', 'V_R_PTS_JJ']\n",
    "procs = [Categorify, Normalize]\n",
    "\n",
    "data = (TabularList.from_df(df, path=\"./data/ligamx\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "   .split_by_idx(range(len(df)-18, len(df)))\n",
    "   .label_from_df(cols=deep_vars)\n",
    "   .databunch(bs=64))\n",
    "\n",
    "learn = tabular_learner(data, layers=[500, 200, 100],  ps=[0.1, 0.1, 0.1], emb_drop=0.1, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='2', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      50.00% [1/2 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.189958</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='10' class='' max='76', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      13.16% [10/76 00:00<00:01 1.3082]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 1.58E-04\n",
      "Min loss divided by 10: 6.92E-03\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.222352</td>\n",
       "      <td>1.144405</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.216577</td>\n",
       "      <td>1.149578</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.200079</td>\n",
       "      <td>1.152589</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.187416</td>\n",
       "      <td>1.162406</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.175572</td>\n",
       "      <td>1.166316</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.162981</td>\n",
       "      <td>1.168123</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.161021</td>\n",
       "      <td>1.158970</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.155203</td>\n",
       "      <td>1.160244</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.149023</td>\n",
       "      <td>1.159206</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.144975</td>\n",
       "      <td>1.150787</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.139945</td>\n",
       "      <td>1.148096</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.131397</td>\n",
       "      <td>1.135213</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.127188</td>\n",
       "      <td>1.143481</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.122666</td>\n",
       "      <td>1.104865</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.115074</td>\n",
       "      <td>1.110662</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.115201</td>\n",
       "      <td>1.087226</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.110256</td>\n",
       "      <td>1.070632</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.105844</td>\n",
       "      <td>1.086256</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.094197</td>\n",
       "      <td>1.077684</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.091777</td>\n",
       "      <td>1.073452</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.089814</td>\n",
       "      <td>1.067418</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.082223</td>\n",
       "      <td>1.052147</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.080758</td>\n",
       "      <td>1.039593</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.076076</td>\n",
       "      <td>1.020214</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.071454</td>\n",
       "      <td>1.008331</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.065692</td>\n",
       "      <td>1.018520</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.063031</td>\n",
       "      <td>1.000161</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.059928</td>\n",
       "      <td>1.000194</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.060221</td>\n",
       "      <td>1.007112</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.051218</td>\n",
       "      <td>0.986414</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.051842</td>\n",
       "      <td>0.984668</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.041779</td>\n",
       "      <td>0.972926</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.041045</td>\n",
       "      <td>0.967396</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.037807</td>\n",
       "      <td>0.945953</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.037971</td>\n",
       "      <td>0.945462</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.032838</td>\n",
       "      <td>0.950707</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.031980</td>\n",
       "      <td>0.946455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.029797</td>\n",
       "      <td>0.941518</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.024492</td>\n",
       "      <td>0.950930</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.021920</td>\n",
       "      <td>0.936390</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.020208</td>\n",
       "      <td>0.957223</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.020289</td>\n",
       "      <td>0.966078</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.016752</td>\n",
       "      <td>0.946245</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.015902</td>\n",
       "      <td>0.924484</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.009868</td>\n",
       "      <td>0.927720</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.001429</td>\n",
       "      <td>0.932061</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.003489</td>\n",
       "      <td>0.927505</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.001779</td>\n",
       "      <td>0.924804</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.999289</td>\n",
       "      <td>0.906648</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.995058</td>\n",
       "      <td>0.889900</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.988921</td>\n",
       "      <td>0.897273</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.992840</td>\n",
       "      <td>0.891771</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.983949</td>\n",
       "      <td>0.898542</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.978200</td>\n",
       "      <td>0.878621</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.980117</td>\n",
       "      <td>0.890135</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.972092</td>\n",
       "      <td>0.865529</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.975857</td>\n",
       "      <td>0.865893</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.980527</td>\n",
       "      <td>0.911817</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.971661</td>\n",
       "      <td>0.861897</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.961219</td>\n",
       "      <td>0.871986</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.962232</td>\n",
       "      <td>0.883570</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.958443</td>\n",
       "      <td>0.886609</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.955378</td>\n",
       "      <td>0.830791</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.949992</td>\n",
       "      <td>0.862268</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.945834</td>\n",
       "      <td>0.880524</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.943235</td>\n",
       "      <td>0.903656</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.941607</td>\n",
       "      <td>0.856833</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.936023</td>\n",
       "      <td>0.817196</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.935768</td>\n",
       "      <td>0.860452</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.933050</td>\n",
       "      <td>0.832983</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.927171</td>\n",
       "      <td>0.830972</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.929798</td>\n",
       "      <td>0.846597</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.925668</td>\n",
       "      <td>0.870608</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.914820</td>\n",
       "      <td>0.843652</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.917765</td>\n",
       "      <td>0.864950</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.914455</td>\n",
       "      <td>0.826385</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.915214</td>\n",
       "      <td>0.860921</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.916546</td>\n",
       "      <td>0.880880</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.912978</td>\n",
       "      <td>0.849244</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.915044</td>\n",
       "      <td>0.853852</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.898208</td>\n",
       "      <td>0.875703</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.896824</td>\n",
       "      <td>0.828150</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.898172</td>\n",
       "      <td>0.838620</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.892722</td>\n",
       "      <td>0.836565</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.887634</td>\n",
       "      <td>0.865024</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.895594</td>\n",
       "      <td>0.871568</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.885096</td>\n",
       "      <td>0.869939</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.886403</td>\n",
       "      <td>0.837140</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.875303</td>\n",
       "      <td>0.859676</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.873953</td>\n",
       "      <td>0.855847</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.863711</td>\n",
       "      <td>0.836890</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.862918</td>\n",
       "      <td>0.862144</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.855446</td>\n",
       "      <td>0.896308</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.867872</td>\n",
       "      <td>0.866898</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.863637</td>\n",
       "      <td>0.861852</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.858273</td>\n",
       "      <td>0.849035</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.858561</td>\n",
       "      <td>0.843171</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.849889</td>\n",
       "      <td>0.874645</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.857483</td>\n",
       "      <td>0.847192</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.845266</td>\n",
       "      <td>0.854537</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.839412</td>\n",
       "      <td>0.838652</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.839183</td>\n",
       "      <td>0.833691</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.841156</td>\n",
       "      <td>0.859981</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.836945</td>\n",
       "      <td>0.854869</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.837850</td>\n",
       "      <td>0.835968</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.834067</td>\n",
       "      <td>0.842773</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.837820</td>\n",
       "      <td>0.862685</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.833086</td>\n",
       "      <td>0.825135</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.821195</td>\n",
       "      <td>0.824843</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.820273</td>\n",
       "      <td>0.847188</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.821413</td>\n",
       "      <td>0.865904</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.832023</td>\n",
       "      <td>0.836637</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.819236</td>\n",
       "      <td>0.858352</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.816276</td>\n",
       "      <td>0.844987</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.817004</td>\n",
       "      <td>0.823167</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.815957</td>\n",
       "      <td>0.830103</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.815947</td>\n",
       "      <td>0.857813</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.818025</td>\n",
       "      <td>0.858604</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.820771</td>\n",
       "      <td>0.852523</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.811381</td>\n",
       "      <td>0.854151</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.805434</td>\n",
       "      <td>0.861670</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.796852</td>\n",
       "      <td>0.847880</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.800907</td>\n",
       "      <td>0.839455</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.806536</td>\n",
       "      <td>0.833017</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.803006</td>\n",
       "      <td>0.851623</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.804195</td>\n",
       "      <td>0.849997</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.799570</td>\n",
       "      <td>0.850605</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.807802</td>\n",
       "      <td>0.849845</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.803586</td>\n",
       "      <td>0.834576</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.806217</td>\n",
       "      <td>0.845894</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.793890</td>\n",
       "      <td>0.823331</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.797890</td>\n",
       "      <td>0.845168</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.794198</td>\n",
       "      <td>0.848339</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.801287</td>\n",
       "      <td>0.837701</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.789470</td>\n",
       "      <td>0.832630</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.791839</td>\n",
       "      <td>0.853668</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.792608</td>\n",
       "      <td>0.809993</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.778497</td>\n",
       "      <td>0.863821</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.790772</td>\n",
       "      <td>0.834921</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.801064</td>\n",
       "      <td>0.842557</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.799191</td>\n",
       "      <td>0.849556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.797375</td>\n",
       "      <td>0.822874</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.798381</td>\n",
       "      <td>0.846377</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.806290</td>\n",
       "      <td>0.823536</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.807534</td>\n",
       "      <td>0.841054</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.795267</td>\n",
       "      <td>0.843981</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.791384</td>\n",
       "      <td>0.852728</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.787443</td>\n",
       "      <td>0.829481</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.795197</td>\n",
       "      <td>0.832030</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.796324</td>\n",
       "      <td>0.843240</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(150, 1e-4, wd=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val_rng = range(len(df)-18, len(df))\n",
    "\n",
    "x = 0\n",
    "for i in val_rng:\n",
    "    dta = df.iloc[i]\n",
    "    pred = learn.predict(dta)\n",
    "    pred = pred[2]\n",
    "    preds = []\n",
    "    for j in range(3):\n",
    "        preds.append((pred[j].item(), j))\n",
    "    preds.sort(reverse = True)\n",
    "    #print(preds)\n",
    "    if(preds[0][1] == dta.RES or preds[1][1] == dta.RES): x+=1\n",
    "    \n",
    "print(x, '/', len(val_rng))\n",
    "x/len(val_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = (TabularList.from_df(sub_data, path=\"./data/ligamx\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "   .split_by_idx(list(range(len(sub_data)-20, len(sub_data))))\n",
    "   .label_from_df(cols=deep_vars)\n",
    "   .databunch(bs=64))\n",
    "\n",
    "learn.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model and learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MatchTabularModel(nn.Module):\n",
    "    \"Basic model for match tabular data.\"\n",
    "    def __init__(self,\n",
    "                 #Contestants parameters\n",
    "                 emb_szs_cts:ListSizes=None, n_cont_cts:int=None, layers_cts:Collection[int]=None, ps_cts:Collection[float]=None, emb_drop_cts:float=0.,\n",
    "                 #General Data parameters\n",
    "                 emb_szs_grl:ListSizes=None, n_cont_grl:int=None, layers_grl:Collection[int]=None, ps_grl:Collection[float]=None, emb_drop_grl:float=0.,\n",
    "                 #General Model parameters\n",
    "                 use_bn:bool=True, bn_final:bool=False, out_sz:int=None, y_range:OptRange=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"Simple Variable Setup -------------------------------------------------------------------------------------\"\"\"\n",
    "        self.contestant_cat_n = len(emb_szs_cts)\n",
    "        self.general_cat_n = len(emb_szs_grl)\n",
    "        self.contestant_cont_n = n_cont_cts\n",
    "        self.general_cont_n = n_cont_grl\n",
    "        \n",
    "        \n",
    "        \"\"\"Contestants Setup -------------------------------------------------------------------------------------\"\"\"\n",
    "        # Dropout probabilities\n",
    "        ps_cts = ifnone(ps_cts, [0]*len(layers_cts))\n",
    "        ps_cts = listify(ps_cts, layers_cts)\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeds_cts = nn.ModuleList([embedding(ni, nf) for ni,nf in emb_szs_cts])\n",
    "        self.emb_drop_cts = nn.Dropout(emb_drop_cts)\n",
    "        n_emb_cts = sum(e.embedding_dim for e in self.embeds_cts)\n",
    "        \n",
    "        # Continious (non-embedding)\n",
    "        self.bn_cont_cts = nn.BatchNorm1d(n_cont_cts)\n",
    "        \n",
    "        # Embeddings and Continious\n",
    "        self.n_emb_cts, self.n_cont_cts = n_emb_cts, n_cont_cts\n",
    "        sizes_cts = self.get_sizes(n_emb_cts, n_cont_cts, layers_cts)\n",
    "        actns_cts = [nn.ReLU(inplace=True) for _ in range(len(sizes_cts)-1)]\n",
    "        layers_cts = []\n",
    "        for i,(n_in, n_out, dp, act) in enumerate(zip(sizes_cts[:-1], sizes_cts[1:], [0.]+ps_cts, actns_cts)):\n",
    "            layers_cts += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "        self.layers_cts = nn.Sequential(*layers_cts)\n",
    "        print(layers_cts)\n",
    "\n",
    "        \"\"\"General Setup -------------------------------------------------------------------------------------\"\"\"\n",
    "        # Dropout Probabilities\n",
    "        ps_grl = ifnone(ps_grl, [0]*len(layers_grl))\n",
    "        ps_grl = listify(ps_grl, layers_grl)\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeds_grl = nn.ModuleList([embedding(ni, nf) for ni,nf in emb_szs_grl])\n",
    "        self.emb_drop_grl = nn.Dropout(emb_drop_grl)\n",
    "        n_emb_grl = sum(e.embedding_dim for e in self.embeds_grl)\n",
    "        \n",
    "        # Continious (non-embedding)\n",
    "        n_cont_grl += sizes_cts[-1]*2 # adding output from contestants subnet to the size of the continious input\n",
    "        self.bn_cont_grl = nn.BatchNorm1d(n_cont_grl)\n",
    "        \n",
    "        # Embeddings and Continitous\n",
    "        self.n_emb_grl, self.n_cont_grl = n_emb_grl, n_cont_grl\n",
    "        sizes_grl = self.get_sizes(n_emb_grl, n_cont_grl, layers_grl, out_sz)\n",
    "        actns_grl = [nn.ReLU(inplace=True) for _ in range(len(sizes_grl)-2)] + [None]\n",
    "        layers_grl = []\n",
    "        for i,(n_in, n_out, dp, act) in enumerate(zip(sizes_grl[:-1], sizes_grl[1:], [0.]+ps_grl, actns_grl)):\n",
    "            layers_grl += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "        \n",
    "        if bn_final: layers_grl.append(nn.BatchNorm1d(sizes_grl[-1]))\n",
    "        self.layers_grl = nn.Sequential(*layers_grl)\n",
    "    \n",
    "\n",
    "        \"\"\"General Model Setup -------------------------------------------------------------------------------------\"\"\"\n",
    "        self.y_range = y_range\n",
    "\n",
    "    def get_sizes(self, n_emb, n_cont, layers, out_sz = None):\n",
    "        res = [n_emb + n_cont] + layers\n",
    "        return  res + [out_sz] if out_sz is not None else res\n",
    "\n",
    "    def forward(self, x_cat:Tensor, x_cont:Tensor) -> Tensor:\n",
    "        contestant_cat = self.contestant_cat_n\n",
    "        general_cat = self.general_cat_n\n",
    "        contestant_cont = self.contestant_cont_n\n",
    "        general_cont = self.general_cont_n\n",
    "\n",
    "        with torch.no_grad():\n",
    "            cat_a = x_cat[:, :contestant_cat].clone()\n",
    "            cat_b = x_cat[:, contestant_cat:contestant_cat*2].clone()\n",
    "            cat_grl = x_cat[:, contestant_cat*2:].clone()\n",
    "            \n",
    "            cont_a = x_cont[:, :contestant_cont].clone()\n",
    "            cont_b = x_cont[:, contestant_cont:contestant_cont*2].clone()\n",
    "            cont_grl = x_cont[:, contestant_cont*2:].clone()\n",
    "        \n",
    "        # Forward of Contestant Layers:\n",
    "        if self.n_emb_cts != 0:\n",
    "            a = [e(cat_a[:,i]) for i,e in enumerate(self.embeds_cts)]\n",
    "            b = [e(cat_b[:,i]) for i,e in enumerate(self.embeds_cts)]\n",
    "            \n",
    "            a = torch.cat(a, 1)\n",
    "            b = torch.cat(b, 1)\n",
    "            \n",
    "            # Not very sure about this step, check here and in layers.\n",
    "            # TODO: implement later: https://discuss.pytorch.org/t/how-to-fix-the-dropout-mask-for-different-batch/7119/3\n",
    "            a = self.emb_drop_cts(a)\n",
    "            b = self.emb_drop_cts(b)\n",
    "        \n",
    "        if self.n_cont_cts != 0:\n",
    "            # not sure about this step, check here and in layers\n",
    "            # TODO: check best practice for the batchnorm (use the same normalization for both or not)\n",
    "            cont_a = self.bn_cont_cts(cont_a)\n",
    "            cont_b = self.bn_cont_cts(cont_b)\n",
    "            \n",
    "            a = torch.cat([a, cont_a], 1) if self.n_emb_cts != 0 else cont_a\n",
    "            b = torch.cat([b, cont_b], 1) if self.n_emb_cts != 0 else cont_b\n",
    "            \n",
    "        if self.n_emb_cts > 0 or self.n_cont_cts > 0:\n",
    "            a = self.layers_cts(a)\n",
    "            b = self.layers_cts(b)\n",
    "            # SUM\n",
    "            x = torch.cat([a, b], 1)\n",
    "        \n",
    "        if self.n_cont_grl != 0:\n",
    "            x = torch.cat([x, cont_grl], 1) if self.n_emb_cts > 0 or self.n_cont_cts else cont_grl\n",
    "            x = self.bn_cont_grl(x)\n",
    "        if self.n_emb_grl != 0:\n",
    "            x_cat = [e(cat_grl[:,i]) for i,e in enumerate(self.embeds_grl)]\n",
    "            x_cat = torch.cat(x_cat, 1)\n",
    "            x_cat = self.emb_drop_grl(x_cat)\n",
    "            x = torch.cat([x, x_cat], 1)\n",
    "            \n",
    "        x = self.layers_grl(x)\n",
    "        if self.y_range is not None:\n",
    "            x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def match_tabular_learner(\n",
    "    #Contestants parameters\n",
    "    layers_cts:Collection[int]=None, emb_szs_cts:Dict[str,int]=None, continious_len_cts:[int]=None, ps_cts:Collection[float]=None, emb_drop_cts:float=0.,\n",
    "    \n",
    "    #General parameters\n",
    "    layers_grl:Collection[int]=None, emb_szs_grl:Dict[str,int]=None, continious_len_grl:[int]=None, ps_grl:Collection[float]=None, emb_drop_grl:float=0.,\n",
    "    \n",
    "    #General\n",
    "    data:DataBunch=None, metrics=None, y_range:OptRange=None, use_bn:bool=True, **learn_kwargs):\n",
    "    \"Get a `Learner` using `data`, with `metrics`, including a `TabularModel` created using the remaining params.\"\n",
    "    \n",
    "    model = MatchTabularModel(\n",
    "        #Contestants parameters\n",
    "        emb_szs_cts, continious_len_cts, layers_cts, ps_cts, emb_drop_cts,\n",
    "        \n",
    "        #General parameters\n",
    "        emb_szs_grl, continious_len_grl, layers_grl, ps_grl, emb_drop_grl,\n",
    "        \n",
    "        #General\n",
    "        use_bn, out_sz = data.c, y_range=y_range)\n",
    "    return Learner(data, model, metrics=metrics, **learn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Use of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "deep_vars = ['RES']\n",
    "#categorical values\n",
    "cat_names = ['LOC',\n",
    "             'VIS',\n",
    "             'JRD', 'ANO', 'TIPO']\n",
    "#continious variables\n",
    "cont_names = ['L_JJ', 'L_POS', 'L_R_JG_JJ', 'L_R_JE_JJ', 'L_R_JP_JJ', 'L_R_GF_JJ', 'L_R_GC_JJ', 'L_R_DIF_JJ', 'L_R_PTS_JJ',\n",
    "              'V_JJ', 'V_POS', 'V_R_JG_JJ', 'V_R_JE_JJ', 'V_R_JP_JJ', 'V_R_GF_JJ', 'V_R_GC_JJ', 'V_R_DIF_JJ', 'V_R_PTS_JJ']\n",
    "procs = [Categorify, Normalize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.iloc[range(154, 174)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Linear(in_features=20, out_features=500, bias=True), ReLU(inplace=True)]\n"
     ]
    }
   ],
   "source": [
    "data = (TabularList.from_df(df, path=\"./data/ligamx\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "   .split_by_idx(range(len(df)-18, len(df)))\n",
    "   .label_from_df(cols=deep_vars)\n",
    "   .databunch(bs=64))\n",
    "learn = match_tabular_learner([500], [(29, 11)], 9, [0.1], 0.1,\n",
    "                              [200], [(21, 9), (17, 8), (3, 3)], 0, [0.1], 0.1,\n",
    "                               data, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=TabularDataBunch;\n",
       "\n",
       "Train: LabelList (4865 items)\n",
       "x: TabularList\n",
       "LOC puebla; VIS tijuana; JRD 1.0; ANO 2019; TIPO apertura; L_JJ -1.6216; L_POS -1.6440; L_R_JG_JJ -1.4181; L_R_JE_JJ -1.2420; L_R_JP_JJ -1.4345; L_R_GF_JJ -1.9524; L_R_GC_JJ -1.9671; L_R_DIF_JJ 0.0493; L_R_PTS_JJ -1.8627; V_JJ -1.6215; V_POS -1.5672; V_R_JG_JJ -1.4556; V_R_JE_JJ -1.2469; V_R_JP_JJ -1.3898; V_R_GF_JJ -1.9557; V_R_GC_JJ -2.0158; V_R_DIF_JJ -0.0662; V_R_PTS_JJ -1.9094; ,LOC atlas; VIS fc juarez; JRD 1.0; ANO 2019; TIPO apertura; L_JJ -1.6216; L_POS -1.6440; L_R_JG_JJ -1.4181; L_R_JE_JJ -1.2420; L_R_JP_JJ -1.4345; L_R_GF_JJ -1.9524; L_R_GC_JJ -1.9671; L_R_DIF_JJ 0.0493; L_R_PTS_JJ -1.8627; V_JJ -1.6215; V_POS -1.5672; V_R_JG_JJ -1.4556; V_R_JE_JJ -1.2469; V_R_JP_JJ -1.3898; V_R_GF_JJ -1.9557; V_R_GC_JJ -2.0158; V_R_DIF_JJ -0.0662; V_R_PTS_JJ -1.9094; ,LOC atletico de san luis; VIS unam; JRD 1.0; ANO 2019; TIPO apertura; L_JJ -1.6216; L_POS -1.6440; L_R_JG_JJ -1.4181; L_R_JE_JJ -1.2420; L_R_JP_JJ -1.4345; L_R_GF_JJ -1.9524; L_R_GC_JJ -1.9671; L_R_DIF_JJ 0.0493; L_R_PTS_JJ -1.8627; V_JJ -1.6215; V_POS -1.5672; V_R_JG_JJ -1.4556; V_R_JE_JJ -1.2469; V_R_JP_JJ -1.3898; V_R_GF_JJ -1.9557; V_R_GC_JJ -2.0158; V_R_DIF_JJ -0.0662; V_R_PTS_JJ -1.9094; ,LOC pachuca; VIS leon; JRD 1.0; ANO 2019; TIPO apertura; L_JJ -1.6216; L_POS -1.6440; L_R_JG_JJ -1.4181; L_R_JE_JJ -1.2420; L_R_JP_JJ -1.4345; L_R_GF_JJ -1.9524; L_R_GC_JJ -1.9671; L_R_DIF_JJ 0.0493; L_R_PTS_JJ -1.8627; V_JJ -1.6215; V_POS -1.5672; V_R_JG_JJ -1.4556; V_R_JE_JJ -1.2469; V_R_JP_JJ -1.3898; V_R_GF_JJ -1.9557; V_R_GC_JJ -2.0158; V_R_DIF_JJ -0.0662; V_R_PTS_JJ -1.9094; ,LOC america; VIS monterrey; JRD 1.0; ANO 2019; TIPO apertura; L_JJ -1.6216; L_POS -1.6440; L_R_JG_JJ -1.4181; L_R_JE_JJ -1.2420; L_R_JP_JJ -1.4345; L_R_GF_JJ -1.9524; L_R_GC_JJ -1.9671; L_R_DIF_JJ 0.0493; L_R_PTS_JJ -1.8627; V_JJ -1.6215; V_POS -1.5672; V_R_JG_JJ -1.4556; V_R_JE_JJ -1.2469; V_R_JP_JJ -1.3898; V_R_GF_JJ -1.9557; V_R_GC_JJ -2.0158; V_R_DIF_JJ -0.0662; V_R_PTS_JJ -1.9094; \n",
       "y: CategoryList\n",
       "0,2,0,0,2\n",
       "Path: data/ligamx;\n",
       "\n",
       "Valid: LabelList (18 items)\n",
       "x: TabularList\n",
       "LOC morelia; VIS santos; JRD 15.0; ANO 2019; TIPO apertura; L_JJ 0.9527; L_POS -0.1993; L_R_JG_JJ 0.5736; L_R_JE_JJ -0.8824; L_R_JP_JJ 0.4763; L_R_GF_JJ 0.2804; L_R_GC_JJ 0.0909; L_R_DIF_JJ 0.1461; L_R_PTS_JJ 0.3137; V_JJ 0.9528; V_POS -1.3864; V_R_JG_JJ 1.1071; V_R_JE_JJ -0.5260; V_R_JP_JJ -0.4035; V_R_GF_JJ 1.6475; V_R_GC_JJ 0.4369; V_R_DIF_JJ 1.0254; V_R_PTS_JJ 0.9972; ,LOC atlas; VIS necaxa; JRD 15.0; ANO 2019; TIPO apertura; L_JJ 1.1508; L_POS -0.3799; L_R_JG_JJ 0.4314; L_R_JE_JJ -0.2403; L_R_JP_JJ 0.0441; L_R_GF_JJ 0.0058; L_R_GC_JJ -0.2809; L_R_DIF_JJ 0.2290; L_R_PTS_JJ 0.3709; V_JJ 1.1508; V_POS -1.2056; V_R_JG_JJ 0.6266; V_R_JE_JJ 0.0920; V_R_JP_JJ -0.4739; V_R_GF_JJ 1.1671; V_R_GC_JJ 0.1418; V_R_DIF_JJ 0.8553; V_R_PTS_JJ 0.6858; ,LOC tijuana; VIS veracruz; JRD 15.0; ANO 2019; TIPO apertura; L_JJ 0.9527; L_POS 0.3425; L_R_JG_JJ 0.2417; L_R_JE_JJ -0.1633; L_R_JP_JJ 0.1578; L_R_GF_JJ 0.7766; L_R_GC_JJ 1.3015; L_R_DIF_JJ -0.4344; L_R_PTS_JJ 0.1991; V_JJ 0.9528; V_POS 1.8683; V_R_JG_JJ -1.4556; V_R_JE_JJ 0.1949; V_R_JP_JJ 1.5691; V_R_GF_JJ -0.9949; V_R_GC_JJ 2.3732; V_R_DIF_JJ -2.6466; V_R_PTS_JJ -1.4622; ,LOC queretaro; VIS unam; JRD 15.0; ANO 2019; TIPO apertura; L_JJ 0.7547; L_POS -0.5605; L_R_JG_JJ 0.7396; L_R_JE_JJ -0.0734; L_R_JP_JJ -0.3995; L_R_GF_JJ 0.8697; L_R_GC_JJ -0.1311; L_R_DIF_JJ 0.7830; L_R_PTS_JJ 0.7432; V_JJ 0.9528; V_POS 0.2409; V_R_JG_JJ 0.1461; V_R_JE_JJ -0.1655; V_R_JP_JJ 0.2540; V_R_GF_JJ -0.2742; V_R_GC_JJ -0.4667; V_R_DIF_JJ 0.1322; V_R_PTS_JJ 0.1029; ,LOC america; VIS puebla; JRD 15.0; ANO 2019; TIPO apertura; L_JJ 1.1508; L_POS -0.7411; L_R_JG_JJ 0.1231; L_R_JE_JJ 1.0952; L_R_JP_JJ -0.8431; L_R_GF_JJ 0.6969; L_R_GC_JJ 0.2811; L_R_DIF_JJ 0.3188; L_R_PTS_JJ 0.4773; V_JJ 0.9528; V_POS 1.6875; V_R_JG_JJ -0.4946; V_R_JE_JJ 0.1949; V_R_JP_JJ 0.5828; V_R_GF_JJ -0.2742; V_R_GC_JJ 1.0824; V_R_DIF_JJ -1.0587; V_R_PTS_JJ -0.4561; \n",
       "y: CategoryList\n",
       "1,0,2,2,2\n",
       "Path: data/ligamx;\n",
       "\n",
       "Test: None, model=MatchTabularModel(\n",
       "  (embeds_cts): ModuleList(\n",
       "    (0): Embedding(29, 11)\n",
       "  )\n",
       "  (emb_drop_cts): Dropout(p=0.1, inplace=False)\n",
       "  (bn_cont_cts): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers_cts): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=500, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (embeds_grl): ModuleList(\n",
       "    (0): Embedding(21, 9)\n",
       "    (1): Embedding(17, 8)\n",
       "    (2): Embedding(3, 3)\n",
       "  )\n",
       "  (emb_drop_grl): Dropout(p=0.1, inplace=False)\n",
       "  (bn_cont_grl): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers_grl): Sequential(\n",
       "    (0): Linear(in_features=1020, out_features=200, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.15, inplace=False)\n",
       "    (4): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): Linear(in_features=100, out_features=3, bias=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f468b68aae8>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('data/ligamx'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): Embedding(29, 11)\n",
       "  (1): Dropout(p=0.1, inplace=False)\n",
       "  (2): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): Linear(in_features=20, out_features=500, bias=True)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): Embedding(21, 9)\n",
       "  (6): Embedding(17, 8)\n",
       "  (7): Embedding(3, 3)\n",
       "  (8): Dropout(p=0.1, inplace=False)\n",
       "  (9): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): Linear(in_features=1020, out_features=200, bias=True)\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): Dropout(p=0.15, inplace=False)\n",
       "  (14): Linear(in_features=200, out_features=100, bias=True)\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): Dropout(p=0.1, inplace=False)\n",
       "  (18): Linear(in_features=100, out_features=3, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='2', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      50.00% [1/2 00:02<00:02]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.231659</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='14' class='' max='76', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      18.42% [14/76 00:00<00:02 2.9298]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='100' class='' max='150', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      66.67% [100/150 04:24<02:12]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.180370</td>\n",
       "      <td>1.085488</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.153889</td>\n",
       "      <td>1.059616</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.140371</td>\n",
       "      <td>1.051938</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.132210</td>\n",
       "      <td>1.067800</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.123822</td>\n",
       "      <td>1.043790</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.112197</td>\n",
       "      <td>1.064460</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.116818</td>\n",
       "      <td>1.054445</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.114223</td>\n",
       "      <td>1.050576</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.111087</td>\n",
       "      <td>1.037667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.102585</td>\n",
       "      <td>1.036324</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.097463</td>\n",
       "      <td>1.025464</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.092827</td>\n",
       "      <td>1.042868</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.088045</td>\n",
       "      <td>1.007547</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.088783</td>\n",
       "      <td>1.033914</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.088011</td>\n",
       "      <td>1.024097</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.077855</td>\n",
       "      <td>1.003967</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.073332</td>\n",
       "      <td>0.989961</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.068224</td>\n",
       "      <td>1.035585</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.067371</td>\n",
       "      <td>0.942052</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.058851</td>\n",
       "      <td>0.990016</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.059699</td>\n",
       "      <td>0.969471</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.054581</td>\n",
       "      <td>0.996180</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.046672</td>\n",
       "      <td>0.972292</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.048867</td>\n",
       "      <td>0.969983</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.045462</td>\n",
       "      <td>1.003657</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.037839</td>\n",
       "      <td>0.964437</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.038059</td>\n",
       "      <td>0.917854</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.030942</td>\n",
       "      <td>0.953627</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.031946</td>\n",
       "      <td>0.963459</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.027165</td>\n",
       "      <td>0.961495</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.026406</td>\n",
       "      <td>0.905863</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.026128</td>\n",
       "      <td>0.940147</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.022719</td>\n",
       "      <td>0.888877</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.021386</td>\n",
       "      <td>0.917825</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.011860</td>\n",
       "      <td>0.877655</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.006412</td>\n",
       "      <td>0.899240</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.001883</td>\n",
       "      <td>0.908829</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.009612</td>\n",
       "      <td>0.891845</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.006561</td>\n",
       "      <td>0.883970</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.002393</td>\n",
       "      <td>0.912134</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.993757</td>\n",
       "      <td>0.893184</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.995346</td>\n",
       "      <td>0.881842</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.993405</td>\n",
       "      <td>0.893631</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.990433</td>\n",
       "      <td>0.913524</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.981266</td>\n",
       "      <td>0.901902</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.987148</td>\n",
       "      <td>0.921104</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.976890</td>\n",
       "      <td>0.907028</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.977821</td>\n",
       "      <td>0.854676</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.976230</td>\n",
       "      <td>0.885829</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.971814</td>\n",
       "      <td>0.892642</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.967280</td>\n",
       "      <td>0.863081</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.966087</td>\n",
       "      <td>0.861015</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.959944</td>\n",
       "      <td>0.869022</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.953482</td>\n",
       "      <td>0.883084</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.955788</td>\n",
       "      <td>0.868715</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.955428</td>\n",
       "      <td>0.909039</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.946614</td>\n",
       "      <td>0.835661</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.948021</td>\n",
       "      <td>0.931120</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.947187</td>\n",
       "      <td>0.910999</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.940999</td>\n",
       "      <td>0.930231</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.938253</td>\n",
       "      <td>0.897565</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.933699</td>\n",
       "      <td>0.929975</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.928272</td>\n",
       "      <td>0.870777</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.929133</td>\n",
       "      <td>0.907722</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.924777</td>\n",
       "      <td>0.886212</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.919203</td>\n",
       "      <td>0.832535</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.921631</td>\n",
       "      <td>0.929592</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.914091</td>\n",
       "      <td>0.905382</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.911857</td>\n",
       "      <td>0.888109</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.910335</td>\n",
       "      <td>0.876654</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.897967</td>\n",
       "      <td>0.941454</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.908121</td>\n",
       "      <td>0.931082</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.903530</td>\n",
       "      <td>0.859648</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.900548</td>\n",
       "      <td>0.886447</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.895597</td>\n",
       "      <td>0.935404</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.885797</td>\n",
       "      <td>0.908363</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.887696</td>\n",
       "      <td>0.908875</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.873998</td>\n",
       "      <td>0.859049</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.870963</td>\n",
       "      <td>0.904231</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.874341</td>\n",
       "      <td>0.854606</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.874545</td>\n",
       "      <td>0.915779</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.872335</td>\n",
       "      <td>0.866961</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.859065</td>\n",
       "      <td>0.846592</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.860805</td>\n",
       "      <td>0.835037</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.852693</td>\n",
       "      <td>0.877392</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.858226</td>\n",
       "      <td>0.876798</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.847751</td>\n",
       "      <td>0.901175</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.849530</td>\n",
       "      <td>0.919066</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.844025</td>\n",
       "      <td>0.877920</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.838147</td>\n",
       "      <td>0.874241</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.843183</td>\n",
       "      <td>0.935129</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.840298</td>\n",
       "      <td>0.892881</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.832194</td>\n",
       "      <td>0.891959</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.825538</td>\n",
       "      <td>0.957210</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.825494</td>\n",
       "      <td>0.875980</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.838876</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.821037</td>\n",
       "      <td>0.945900</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.806647</td>\n",
       "      <td>0.914634</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.810720</td>\n",
       "      <td>0.970285</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.803860</td>\n",
       "      <td>0.907216</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='65' class='' max='76', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      85.53% [65/76 00:01<00:00 0.8041]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-07da01cb208e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     22\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m def fit_fc(learn:Learner, tot_epochs:int=1, lr:float=defaults.lr,  moms:Tuple[float,float]=(0.95,0.85), start_pct:float=0.72,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_bwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_bwd\u001b[0m\u001b[0;34m:\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = learn.fit_one_cycle(150, 1e-4, wd=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# learn.save('o2')\n",
    "# learn.load('o2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, 6e-7, wd=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#sub_data = pd.concat([data_separated[6], data_separated[7]])\n",
    "\n",
    "val_rng = range(len(sub_data)-20, len(sub_data))\n",
    "\n",
    "data = (TabularList.from_df(sub_data, path=\"./data/ligamx\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "   .split_by_idx(val_rng)\n",
    "   .label_from_df(cols=deep_vars)\n",
    "   .databunch(bs=64))\n",
    "learn.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val_rng = range(152, 167)\n",
    "df.iloc[val_rng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.iloc[range(154, 174)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sub_data.reset_index(drop=True, inplace=True)\n",
    "sub_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.fit(10, 1e-20, wd=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# learn.save('MatchPred1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(data_lmx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val_rng = range(152, 161)\n",
    "\n",
    "x = 0\n",
    "for i in val_rng:\n",
    "    dta = df.iloc[i]\n",
    "    pred = learn.predict(dta)\n",
    "    pred = pred[2]\n",
    "    preds = []\n",
    "    for j in range(3):\n",
    "        preds.append((pred[j].item(), j))\n",
    "    preds.sort(reverse = True)\n",
    "    #print(preds)\n",
    "    if(preds[0][1] == dta.RES or preds[1][1] == dta.RES): x+=1\n",
    "    \n",
    "print(x, '/', len(val_rng))\n",
    "x/len(val_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = (TabularList.from_df(df, path=\"./data/ligamx\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "   .split_by_idx(range(154, 174))\n",
    "   .label_from_df(cols=deep_vars)\n",
    "   .databunch(bs=64))\n",
    "learn.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, 1e-1000, wd=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Iterative training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = (TabularList.from_df(df, path=\"./data/ligamx\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "   .split_none()\n",
    "   .label_from_df(cols=deep_vars)\n",
    "   .databunch(bs=64))\n",
    "\n",
    "learn = match_tabular_learner([200, 100, 50], [(27, 50), (3, 5)], 9, None, 0.,\n",
    "                               [100, 50], [(21, 9), (9, 5), (3, 3)], 0, None, 0.,\n",
    "                               data, metrics=accuracy)\n",
    "\n",
    "for dta in data_separated:\n",
    "    data = (TabularList.from_df(dta, path=\"./data/ligamx\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "   .split_by_idx(range(len(dta)-20, len(dta)))\n",
    "   .label_from_df(cols=deep_vars)\n",
    "   .databunch(bs=45))\n",
    "    learn.data = data\n",
    "    learn.fit(7, 1e-3, wd=0.4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Several tryouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = (TabularList.from_df(df, path=\"./data/ligamx\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "   .split_by_idx(range(134, 174))\n",
    "   .label_from_df(cols=deep_vars)\n",
    "   .databunch(bs=64))\n",
    "learn = match_tabular_learner([350, 150], [(29, 15), (3, 3)], 9, [0.3, 0.3], 0.3,\n",
    "                               [100], [(21, 9), (17, 8), (3, 3)], 0, [0.3], 0.3,\n",
    "                               data, metrics=accuracy)\n",
    "learn.fit_one_cycle(200, 1e-4, wd=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = (TabularList.from_df(df, path=\"./data/ligamx\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "   .split_by_idx(range(154, 174))\n",
    "   .label_from_df(cols=deep_vars)\n",
    "   .databunch(bs=64))\n",
    "learn = match_tabular_learner([200, 100], [(29, 15), (3, 3)], 9, [0.3, 0.3], 0.3,\n",
    "                               [70], [(21, 9), (17, 8), (3, 3)], 0, [0.3], 0.3,\n",
    "                               data, metrics=accuracy)\n",
    "learn.fit_one_cycle(200, 1e-4, wd=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = (TabularList.from_df(df, path=\"./data/ligamx\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "   .split_by_idx(range(154, 174))\n",
    "   .label_from_df(cols=deep_vars)\n",
    "   .databunch(bs=64))\n",
    "learn = match_tabular_learner([500], [(29, 15), (3, 3)], 9, [0.2], 0.2,\n",
    "                               [100], [(21, 9), (17, 8), (3, 3)], 0, [0.2], 0.2,\n",
    "                               data, metrics=accuracy)\n",
    "learn.fit_one_cycle(200, 1e-4, wd=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = (TabularList.from_df(df, path=\"./data/ligamx\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "   .split_by_idx(range(154, 174))\n",
    "   .label_from_df(cols=deep_vars)\n",
    "   .databunch(bs=64))\n",
    "learn = match_tabular_learner([550], [(29, 15), (3, 3)], 9, [0.2], 0.2,\n",
    "                               [70], [(21, 9), (17, 8), (3, 3)], 0, [0.2], 0.2,\n",
    "                               data, metrics=accuracy)\n",
    "learn.fit_one_cycle(200, 1e-4, wd=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = (TabularList.from_df(df, path=\"./data/ligamx\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "   .split_by_idx(range(134, 174))\n",
    "   .label_from_df(cols=deep_vars)\n",
    "   .databunch(bs=64))\n",
    "learn = match_tabular_learner([300], [(29, 15), (3, 3)], 9, [0.2], 0.2,\n",
    "                               [300], [(21, 9), (17, 8), (3, 3)], 0, [0.2], 0.2,\n",
    "                               data, metrics=accuracy)\n",
    "learn.fit_one_cycle(200, 1e-4, wd=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Trying regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tryRegular():\n",
    "    deep_vars = ['RES']\n",
    "    #categorical values\n",
    "    cat_names = ['LOC', 'VIS', 'JRD', 'ANO', 'TIPO']\n",
    "    #continious variables\n",
    "    cont_names = ['L_JJ', 'L_POS', 'V_POS',\n",
    "           'L_R_JG_JJ', 'L_R_JE_JJ', 'L_R_JP_JJ', 'L_R_GF_JJ', 'L_R_GC_JJ',\n",
    "           'L_R_DIF_JJ', 'L_R_PTS_JJ', 'V_JJ', 'V_R_JG_JJ', 'V_R_JE_JJ',\n",
    "           'V_R_JP_JJ', 'V_R_GF_JJ', 'V_R_GC_JJ', 'V_R_DIF_JJ', 'V_R_PTS_JJ']\n",
    "    procs = [Categorify, Normalize]\n",
    "\n",
    "    data = (TabularList.from_df(df, path=\"./data/ligamx\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "       .split_by_idx(list(range(152, 161)))\n",
    "       .label_from_df(cols=deep_vars)\n",
    "       .databunch(bs=64))\n",
    "\n",
    "    learn = tabular_learner(data, layers=[500, 200, 100],  ps=[0.1, 0.1, 0.1], emb_drop=0.1, metrics=accuracy)\n",
    "    learn.fit_one_cycle(150, 1e-4, wd=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    tryRegular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tr = [0.801, 0.815, 0.811, 0.816, 0.791, 0.787, 0.797, 0.800, 0.790, 0.790]\n",
    "vl = [0.788, 0.757, 0.756, 0.776, 0.800, 0.839, 0.681, 0.800, 0.681, 0.71]\n",
    "tr = tensor(tr)\n",
    "vl = tensor(vl)\n",
    "\n",
    "print(tr.mean(), tr.std())\n",
    "print(vl.mean(), vl.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Try MatchPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def try_match_pred():\n",
    "    deep_vars = ['RES']\n",
    "    #categorical values\n",
    "    cat_names = ['LOC',\n",
    "                 'VIS',\n",
    "                 'JRD', 'ANO', 'TIPO']\n",
    "    #continious variables\n",
    "    cont_names = ['L_JJ', 'L_POS', 'L_R_JG_JJ', 'L_R_JE_JJ', 'L_R_JP_JJ', 'L_R_GF_JJ', 'L_R_GC_JJ', 'L_R_DIF_JJ', 'L_R_PTS_JJ',\n",
    "                  'V_JJ', 'V_POS', 'V_R_JG_JJ', 'V_R_JE_JJ', 'V_R_JP_JJ', 'V_R_GF_JJ', 'V_R_GC_JJ', 'V_R_DIF_JJ', 'V_R_PTS_JJ']\n",
    "    procs = [Categorify, Normalize]\n",
    "\n",
    "    data = (TabularList.from_df(df, path=\"./data/ligamx\", cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "       .split_by_idx(range(152, 161))\n",
    "       .label_from_df(cols=deep_vars)\n",
    "       .databunch(bs=64))\n",
    "    learn = match_tabular_learner([90, 20], [(29, 15)], 9, [0.1, 0.1], 0.1,\n",
    "                                   [500, 200, 100], [(21, 9), (17, 8), (3, 3)], 0, [0.1, 0.1, 0.1], 0.1,\n",
    "                                   data, metrics=accuracy)\n",
    "\n",
    "    learn.fit_one_cycle(200, 1e-4, wd=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    try_match_pred()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
